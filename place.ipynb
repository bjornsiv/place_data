{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "chunk_size = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits up the dataset into chunks of 500,000 rows each\n",
    "for i,chunk in enumerate(pd.read_csv('2022_place_canvas_history.csv.gzip', chunksize=chunk_size, compression='gzip')):\n",
    "   chunk.to_csv('./tmp/split_csv_pandas/chunk{}.csv'.format(i), index=False)\n",
    "print('The chunks have been written to ./tmp/split_csv_pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset is:  160353104  rows long\n"
     ]
    }
   ],
   "source": [
    "#Counts how many rows is in the dataset and prints it, must be run for the rest to work\n",
    "path = './tmp/split_csv_pandas'\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "len = 0\n",
    "counter=-1\n",
    "sum=0\n",
    "\n",
    "for i, df_chunk in enumerate(csv_files):\n",
    "    sum += chunk_size\n",
    "    counter += 1\n",
    "sum -= chunk_size\n",
    "\n",
    "last_path = './tmp/split_csv_pandas/chunk' + str(counter) + '.csv'\n",
    "last_len = pd.read_csv(last_path).shape[0]\n",
    "sum += last_len\n",
    "\n",
    "print('This dataset is: ', str(sum), ' rows long')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cells under will generate a csv file with user_id and occurences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops throuh the entire dataset and count how many occurences of a specific user_id\n",
    "for i, df_chunk in enumerate(csv_files):\n",
    "    df_temp = pd.read_csv(df_chunk)\n",
    "\n",
    "    freq = df_temp.groupby(['user_id']).size()\n",
    "    freq.to_csv('./tmp/user_list_split_csv/chunk{}.csv'.format(i), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines every user_list_csv chunk into one dataframe, Loooooooooooooooooooong runtime\n",
    "path_user_list = './tmp/user_list_split_csv'\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "len_sum = 0\n",
    "user_list = pd.DataFrame()\n",
    "for i, df_user in enumerate(csv_files):\n",
    "    user_list_temp = pd.read_csv(df_user, sep=',', encoding='utf-8')\n",
    "    len_sum += user_list_temp.shape[0]\n",
    "    user_list = pd.concat([user_list, freq])\n",
    "\n",
    "print('The dataset contains ', str(len_sum), ' unique user_id s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes the huuuge dataframe to a csv file\n",
    "user_list.to_csv('./results/user_list.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i will do the same for coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops throuh the entire dataset and count how many occurences of a specific coordinate\n",
    "for i, df_chunk in enumerate(csv_files):\n",
    "    df_temp = pd.read_csv(df_chunk)\n",
    "\n",
    "    freq = df_temp.groupby(['coordinate']).size()\n",
    "    freq.to_csv('./tmp/coordinate_list_split_csv/chunk{}.csv'.format(i), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines every coordinate_csv chunk into one file, Loooooooooooooooooooong runtime\n",
    "path_coordinate_list = './tmp/coordinate_list_split_csv'\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "len_sum = 0\n",
    "coordinate = pd.DataFrame()\n",
    "for i, df_user in enumerate(csv_files):\n",
    "    coordinate_temp = pd.read_csv(df_user, sep=',', encoding='utf-8')\n",
    "    len_sum += coordinate_temp.shape[0]\n",
    "    coordinate = pd.concat([coordinate, freq])\n",
    "    print('This is ', i)\n",
    "print('The dataset contains ', str(len_sum), ' unique coordinate s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skriver ut chunkene til en csv fil, kinda pointless :/\n",
    "df = pd.concat((chunk for chunk in pd.read_csv('2022_place_canvas_history.csv',chunksize=1000000)))\n",
    "df.to_csv('./results/out.csv', sep=',', encoding='utf-8', index=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73284ed3e2e97ecf70ec9a50d0d70f3426d084330c5b21b3e3e82720657c6dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
